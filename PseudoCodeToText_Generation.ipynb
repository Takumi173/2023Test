{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Takumi173/2023Test/blob/main/PseudoCodeToText_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DkL8RCl_Jgz"
      },
      "source": [
        "# プログラム"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NIIx3W3LKb4"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/Takumi173/2023Test/releases/download/20231114/TestData_Ver.0.4.csv\n",
        "!wget https://github.com/Takumi173/2023Test/releases/download/20231114/TrainData_Ver.0.4.csv\n",
        "!wget https://github.com/Takumi173/2023Test/releases/download/20231126/PreparedData.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXOLvnF9i5Bl"
      },
      "outputs": [],
      "source": [
        "# パッケージのインストール\n",
        "!pip install transformers\n",
        "#!pip install git+https://github.com/huggingface/transformers -Uqq #for mistral\n",
        "!pip install sentencepiece accelerate bitsandbytes sentence_transformers xformers\n",
        "\n",
        "!pip install einops # for japanese-stablelm-instruct-alpha-7b\n",
        "\n",
        "#AutoGPTQ\n",
        "!pip install optimum\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgJzPAojJE-G"
      },
      "outputs": [],
      "source": [
        "# HuggingFaceのログイン\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-ZMx7GuxRyq"
      },
      "source": [
        "# llama2 Chat方式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtgfyHo_PFKX"
      },
      "outputs": [],
      "source": [
        "from threading import Thread\n",
        "from typing import Iterator\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig\n",
        "\n",
        "#model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'  #13Bの4bitのほうが7bのフルロードよりも正確。知識ベースの積み上げはモデルが大きいほど正確っぽい。Bitが影響するのは書き出しの部分か？'\n",
        "#model_id = 'codellama/CodeLlama-13b-Instruct-hf'\n",
        "#model_id = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
        "#model_id = 'pfnet/plamo-13b'\n",
        "#model_id = 'mistralai/Mistral-7B-Instruct-v0.1' メモリ不足\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "    if model_id in ('meta-llama/Llama-2-13b-chat-hf', 'pfnet/plamo-13b', 'codellama/CodeLlama-13b-Instruct-hf'):\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_id,\n",
        "          torch_dtype=torch.float16,\n",
        "          device_map='auto',\n",
        "#          load_in_8bit=True,\n",
        "          quantization_config=bnb_config\n",
        "      )\n",
        "    elif model_id in ('elyza/ELYZA-japanese-Llama-2-7b-instruct'):\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_id,\n",
        "          torch_dtype=torch.float16,\n",
        "          device_map='auto',\n",
        "      )\n",
        "    elif model_id in ('mistralai/Mistral-7B-Instruct-v0.1'):\n",
        "      bnb_config  = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "      )\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_id,\n",
        "          trust_remote_code=True,\n",
        "#          torch_dtype=torch.float16,\n",
        "          device_map='auto',\n",
        "          quantization_config=bnb_config\n",
        "      )\n",
        "    else:\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_id,\n",
        "          torch_dtype=torch.float16,\n",
        "          device_map='auto',\n",
        "      )\n",
        "else:\n",
        "    model = None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy0IJZLEbi4D"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def flush():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.reset_peak_memory_stats()\n",
        "  return\n",
        "\n",
        "def GeneratePrediction (Code):\n",
        "  flush()\n",
        "\n",
        "  prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful, honest and excellent programmer.  Always answer as helpfully as possible, while being safe.  Please answer the questions as accurately as possible based on the knowledge you have.  Take a deep breath, read the text carefully, and think step-by-step!\n",
        "<</SYS>>\n",
        "\n",
        "### CONTEXT\n",
        "Here are examples of translations from CODE to TEXT.\n",
        "\n",
        "Example 1:\n",
        "CODE: [Overall Response] <> \"PROGRESSIVE DISEASE\" AND [Overall Response] IS NOT EMPTY AND [New Lesion Progression] = \"UNEQUIVOCAL\"\n",
        "TEXT: The [Overall Response] is not \"PROGRESSIVE DISEASE\", yet [New Lesion Progression] is \"UNEQUIVOCAL\".  Please review.\n",
        "\n",
        "Example 2:\n",
        "CODE: [Stop Date] > [Date of First Study Drug Taken] -30\n",
        "TEXT: The [Stop Date] is more than 30 days prior to the [Date of First Study Drug Taken].  Please review.\n",
        "\n",
        "Example 3:\n",
        "CODE: [Primary Study Drug Treatment Status] = \"COMPLETED\" and [Date of Last Dose of Study Drug] < [Study Day 90]\n",
        "TEXT: The [Primary Study Drug Treatment Status] is \"COMPLETED\", yet the [Date of Last Dose of Study Drug] is prior to [Study Day 90].  Please review.\n",
        "\n",
        "Example 4:\n",
        "CODE: 18 > [Age]\n",
        "TEXT: [Age] is less than 18 years.\n",
        "\n",
        "Example 5:\n",
        "CODE: [LBTEST] = \"Leukocytes\" AND ([LBORRES] < 13 or [LBORRES] > 294)\n",
        "TEXT: The [LBTEST] is \"Leukocytes\" and the [LBORRES] is not within the expected range of 13-294. Please review.\n",
        "\n",
        "Example 6:\n",
        "CODE: [Do you consider that there is a reasonable possibility that the event may have been caused by study drug?] = \"YES\" AND [Start date] < [Date of First Study Drug Taken]\n",
        "TEXT: The [Do you consider that there is a reasonable possibility that the event may have been caused by study drug?] is \"YES\" and the [Start Date] is prior to the [Date of First Study Drug Taken].  Please review.\n",
        "\n",
        "Example 7:\n",
        "CODE: Record count of [Subject ID] is more than 1\n",
        "TEXT: Duplicate [Subject ID].  Please review.\n",
        "\n",
        "\n",
        "### QUESTION\n",
        "\n",
        "Step1: Referring to the CONTEXT examples, translate the following CODE into TEXT.  It is important to maintain the CODE and the TEXT square brackets or quotation marks around the text when translating.\n",
        "CODE: {Code}\n",
        "\n",
        "Step2: Check if the text enclosed in square brackets or quotation marks in the CODE and the TEXT match exactly.  Also check that the logic of the CODE and the TEXT match exactly.  If not, modify the TEXT to match.  Answer should be start with \"TEXT:\" and end with \"Please review.\"  No explanation required.\n",
        "\n",
        "\n",
        "### ANSWER\n",
        "TEXT: [/INST]\"\"\"\n",
        "\n",
        "\n",
        "  #print(prompt)\n",
        "  with torch.no_grad():\n",
        "      token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "      if len(token_ids[0]) > 1500:\n",
        "        output = \"Too many text to read\"\n",
        "        return prompt, output\n",
        "\n",
        "      output_ids = model.generate(\n",
        "          token_ids.to(model.device),\n",
        "          max_new_tokens=256,\n",
        "          do_sample=False,\n",
        "  #        top_p=1,\n",
        "  #        top_k=1,\n",
        "  #        temperature=0.1,\n",
        "  #        pad_token_id=tokenizer.pad_token_id,\n",
        "  #        eos_token_id=tokenizer.eos_token_id,\n",
        "      )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "  return prompt, output\n",
        "\n",
        "Code = '[If Pregnancy Test was not done, please provide reason] <> \"PRE-PUBERTY\" or [Age] < 10'\n",
        "\n",
        "prompt, output = GeneratePrediction(Code)\n",
        "\n",
        "print(\"*** OUTPUT ***\")\n",
        "print(output)\n",
        "print(\"\\n*** PROMPT ***\")\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('TestData_Ver.0.4.csv')\n",
        "df1[\"Origin\"] = \"TestData\"\n",
        "df2 = pd.read_csv('TrainData_Ver.0.4.csv')\n",
        "df2[\"Origin\"] = \"TrainData\"\n",
        "\n",
        "df = pd.concat([df1,df2],axis = 0)\n",
        "df"
      ],
      "metadata": {
        "id": "59Ur0l2f28qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_zvP9vYbiwx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "ResList = []\n",
        "PredList = []\n",
        "\n",
        "for t in tqdm(df['Code'].tolist()):\n",
        "  _, Response = GeneratePrediction(t)\n",
        "  PredictedText = Response.split(\"TEXT:\")[-1].split(\"\\n\")[0].strip()\n",
        "\n",
        "  print(f'Generated *** {PredictedText}')\n",
        "  ResList.append(Response)\n",
        "  PredList.append(PredictedText)\n",
        "\n",
        "df_pred = df\n",
        "df_pred['Response'] = ResList\n",
        "df_pred['GeneratedText'] = PredList\n",
        "\n",
        "df_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred.to_csv('PredData_llama2.csv')\n",
        "#df_pred.to_csv('PredData_codellama.csv')"
      ],
      "metadata": {
        "id": "87AfCNnHEbPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "h8I4l2JQpw4n",
        "Er0pj8u7prbK"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}